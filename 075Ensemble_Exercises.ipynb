{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ewakobrzynska/MachineLearning/blob/main/075Ensemble_Exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weR9J-_OamV1"
      },
      "source": [
        "# Ensemble methods. Exercises\n",
        "\n",
        "\n",
        "In this section we have only two exercise:\n",
        "\n",
        "1. Find the best three classifier in the stacking method using the classifiers from scikit-learn package.\n",
        "\n",
        "2. Build arcing arc-x4 method."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "data_set = iris.data[0:len(iris.target)-20,:]\n",
        "labels = iris.target[0:len(iris.target)-20]\n",
        "unique_labels = np.unique(iris.target)\n",
        "\n",
        "test_data_set = iris.data[-20:,:]\n",
        "test_labels = iris.target[-20:]"
      ],
      "metadata": {
        "id": "9TLGTo-5az3h"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gA5hlcvYamV3"
      },
      "source": [
        "## Exercise 1: Find the best three classifier in the stacking method\n",
        "\n",
        "Please use the following classifiers:\n",
        "\n",
        "* Linear regression,\n",
        "* Nearest Neighbors,\n",
        "* Linear SVM,\n",
        "* Decision Tree,\n",
        "* Naive Bayes,\n",
        "* QDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "vn0V6txyamV3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "pBZ86C_famV3"
      },
      "outputs": [],
      "source": [
        "# Define the base classifiers\n",
        "base_classifiers = [\n",
        "    ('lr', LinearRegression()),\n",
        "    ('knn', KNeighborsClassifier()),\n",
        "    ('svm', SVC()),\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('nb', GaussianNB()),\n",
        "    ('qda', QuadraticDiscriminantAnalysis())\n",
        "]\n",
        "\n",
        "def build_classifiers():\n",
        "    classifiers = {}\n",
        "    for name, clf in base_classifiers:\n",
        "        classifiers[name] = clf\n",
        "    return classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "tWgqBr48amV3"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "def build_stacked_classifier(classifiers):\n",
        "    output = []\n",
        "    for classifier in classifiers.values():\n",
        "        classifier.fit(data_set, labels)\n",
        "    for classifier in classifiers.values():\n",
        "        output.append(classifier.predict(data_set))\n",
        "    output = np.array(output).T\n",
        "    stacked_classifier = LogisticRegression()\n",
        "    stacked_classifier.fit(output, labels)\n",
        "    test_output = []\n",
        "    for classifier in classifiers.values():\n",
        "        test_output.append(classifier.predict(test_data_set))\n",
        "    test_output = np.array(test_output).T\n",
        "    predicted = stacked_classifier.predict(test_output)\n",
        "    return predicted\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJ6uF7LqamV3",
        "outputId": "ece09e86-caff-4bea-e3ce-d86fb36d9898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.95\n"
          ]
        }
      ],
      "source": [
        "classifiers = build_classifiers()\n",
        "predicted = build_stacked_classifier(classifiers)\n",
        "accuracy = accuracy_score(test_labels, predicted)\n",
        "print(accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkeiIcc8amV3"
      },
      "source": [
        "## Exercise 2:\n",
        "\n",
        "Use the boosting method and change the code to fullfilt the following requirements:\n",
        "\n",
        "* the weights should be calculated as:\n",
        "$w_{n}^{(t+1)}=\\frac{1+ I(y_{n}\\neq h_{t}(x_{n})}{\\sum_{i=1}^{N}1+I(y_{n}\\neq h_{t}(x_{n})}$,\n",
        "* the prediction is done with a voting method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G2abviDAamV3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# prepare data set\n",
        "\n",
        "def generate_data(sample_number, feature_number, label_number):\n",
        "    data_set = np.random.random_sample((sample_number, feature_number))\n",
        "    labels = np.random.choice(label_number, sample_number)\n",
        "    return data_set, labels\n",
        "\n",
        "labels = 2\n",
        "dimension = 2\n",
        "test_set_size = 1000\n",
        "train_set_size = 5000\n",
        "train_set, train_labels = generate_data(train_set_size, dimension, labels)\n",
        "test_set, test_labels = generate_data(test_set_size, dimension, labels)\n",
        "\n",
        "# init weights\n",
        "number_of_iterations = 10\n",
        "weights = np.ones((test_set_size,)) / test_set_size\n",
        "\n",
        "\n",
        "def train_model(classifier, weights):\n",
        "    return classifier.fit(X=test_set, y=test_labels, sample_weight=weights)\n",
        "\n",
        "def calculate_error(model):\n",
        "    predicted = model.predict(test_set)\n",
        "    I=calculate_accuracy_vector(predicted, test_labels)\n",
        "    Z=np.sum(I)\n",
        "    return (1+Z)/1.0\n",
        "\n",
        "#z notatnika '074Ensemble_Stacking.ipynb'\n",
        "def calculate_accuracy_vector(predicted, labels):\n",
        "    result = []\n",
        "    for i in range(len(predicted)):\n",
        "        if predicted[i] == labels[i]:\n",
        "            result.append(1)\n",
        "        else:\n",
        "            result.append(0)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR9KX7m5amV4"
      },
      "source": [
        "Fill the two functions below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "O98ywOXPamV4"
      },
      "outputs": [],
      "source": [
        "def set_new_weights(model):\n",
        "    # fill the code here (two lines)\n",
        "    I = np.array(calculate_accuracy_vector(model.predict(test_set), test_labels))\n",
        "    weights = (1 + I) / (1 + I).sum()\n",
        "    return weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nQ_NVxMamV4"
      },
      "source": [
        "Train the classifier with the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gl3ALor4amV4",
        "outputId": "7d03d814-12e1-4f52-eb0a-41fec16d31dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00129786 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00064893 0.00129786 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893\n",
            " 0.00129786 0.00064893 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00064893 0.00064893 0.00064893 0.00064893\n",
            " 0.00129786 0.00064893 0.00064893 0.00064893 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00064893 0.00064893 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00064893 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00064893 0.00064893\n",
            " 0.00129786 0.00064893 0.00064893 0.00064893 0.00064893 0.00064893\n",
            " 0.00064893 0.00064893 0.00064893 0.00129786 0.00064893 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00064893 0.00064893 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893 0.00064893\n",
            " 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00064893 0.00064893 0.00064893 0.00064893\n",
            " 0.00064893 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00064893 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00129786 0.00064893 0.00064893\n",
            " 0.00064893 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00064893 0.00064893 0.00064893 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00129786 0.00064893 0.00064893 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00064893 0.00064893 0.00064893\n",
            " 0.00064893 0.00064893 0.00129786 0.00064893 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00064893 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00064893 0.00064893 0.00064893\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893\n",
            " 0.00064893 0.00064893 0.00064893 0.00064893 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00064893 0.00064893 0.00064893 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893 0.00064893\n",
            " 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00064893 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00129786 0.00064893 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00064893 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00064893 0.00064893 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00129786 0.00129786 0.00064893 0.00129786 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00064893 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00064893 0.00064893 0.00064893 0.00064893 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786\n",
            " 0.00129786 0.00129786 0.00064893 0.00129786 0.00064893 0.00064893\n",
            " 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00129786 0.00129786 0.00064893 0.00064893\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786 0.00064893\n",
            " 0.00064893 0.00064893 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00064893 0.00064893 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00129786 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893 0.00129786\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893\n",
            " 0.00064893 0.00129786 0.00064893 0.00064893 0.00064893 0.00064893\n",
            " 0.00129786 0.00064893 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00064893 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00064893 0.00064893\n",
            " 0.00064893 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893\n",
            " 0.00064893 0.00129786 0.00064893 0.00064893 0.00064893 0.00064893\n",
            " 0.00064893 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00064893 0.00064893\n",
            " 0.00129786 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00064893 0.00064893 0.00129786 0.00064893 0.00129786\n",
            " 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893 0.00064893\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786\n",
            " 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893 0.00129786\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00129786 0.00064893 0.00064893 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786 0.00064893\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786 0.00129786\n",
            " 0.00064893 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893\n",
            " 0.00129786 0.00129786 0.00064893 0.00064893 0.00129786 0.00129786\n",
            " 0.00064893 0.00129786 0.00129786 0.00129786 0.00064893 0.00129786\n",
            " 0.00064893 0.00129786 0.00064893 0.00129786 0.00129786 0.00064893\n",
            " 0.00064893 0.00064893 0.00129786 0.00129786]\n"
          ]
        }
      ],
      "source": [
        "classifier = DecisionTreeClassifier(max_depth=1, random_state=1)\n",
        "classifier.fit(X=train_set, y=train_labels)\n",
        "alphas = []\n",
        "classifiers = []\n",
        "for iteration in range(number_of_iterations):\n",
        "    model = train_model(classifier, weights)\n",
        "    weights = set_new_weights(model)\n",
        "    classifiers.append(model)\n",
        "\n",
        "print(weights)\n",
        "\n",
        "\n",
        "validate_x, validate_label = generate_data(1, dimension, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHanqrWsamV4"
      },
      "source": [
        "Set the validation data set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "MMXfUQ8JamV4"
      },
      "outputs": [],
      "source": [
        "validate_x, validate_label = generate_data(1, dimension, labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TE_XGROVamV4"
      },
      "source": [
        "Fill the prediction code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "xVUtB-ktamV4"
      },
      "outputs": [],
      "source": [
        "def get_prediction(x):\n",
        "    output = np.array([classifier.predict(x) for classifier in classifiers])\n",
        "    predicted = np.argmax(np.bincount(output.flatten()))\n",
        "    return [predicted]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-AKS3tKamV4"
      },
      "source": [
        "Test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0LtyvygamV4",
        "outputId": "10407a9b-fbdc-47a0-e74c-277bd764e9a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ],
      "source": [
        "prediction = get_prediction(validate_x)[0]\n",
        "print(prediction)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}